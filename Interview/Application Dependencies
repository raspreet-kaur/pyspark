Bundling Your Application’s Dependencies
If your code depends on other projects, you will need to package them alongside your application in order to distribute the code to a 
Spark cluster.To do this, to create an assembly jar (or “uber” jar) containing your code and its dependencies. Both sbt and Maven have 
assembly plugins.
When creating assembly jars, list Spark and Hadoop as provided dependencies; these need not be bundled since they are provided by the
cluster manager at runtime. Once you have an assembled jar you can call the bin/spark-submit script as shown here while passing your jar.

For Python, you can use the --py-files argument of spark-submit to add .py, .zip or .egg files to be distributed with your application.
If you depend on multiple Python files we recommend packaging them into a .zip or .egg.

https://spark.apache.org/docs/1.0.2/submitting-applications.html

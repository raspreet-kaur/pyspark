When a client submits a spark user application code, 
	the driver implicitly converts the code containing transformations and actions into a logical directed acyclic graph (DAG). 
	At this stage, the driver program also performs certain optimizations like pipelining transformations and
  then it converts the logical DAG into physical execution plan with set of stages.
	After creating the physical execution plan, it creates small physical execution units referred to as tasks under each stage.
  Then tasks are bundled to be sent to the Spark Cluster.

	The driver program then talks to the cluster manager and negotiates for resources. 
	The cluster manager then launches executors on the worker nodes on behalf of the driver.
	At this point the driver sends tasks to the cluster manager based on data placement. 
	Before executors begin execution, they register themselves with the driver program 
  so that the driver has holistic view of all the executors.
	Now executors start executing the various tasks assigned by the driver program. 
	At any point of time when the spark application is running,
  the driver program will monitor the set of executors that run.
	Driver program in the spark architecture also schedules future tasks based on data placement by tracking the location of cached data.
	When driver programs main () method exits or when it call the stop () method of the Spark Context,
  it will terminate all the executors and release the resources from the cluster manager.

The structure of a Spark program at higher level is -
RDD's are created from the input data and new RDD's are derived from the existing RDD's using different transformations,
after which an action is performed on the data.
In any spark program, the DAG operations are created by default and
whenever the driver runs the Spark DAG will be converted into a physical execution plan.

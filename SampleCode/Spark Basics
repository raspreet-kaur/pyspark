Spark is nothing but distributed computing framework.
To leverage the framework we need to learn API categorized into different modules
and build applications using supported programming languages (like Scala, Python, Java etc).

Let us have a quick review of APIs that are available in Spark.
1-SparkContext exposes APIs such as textFile, sequenceFile to read data from files into a distributed collection called as RDD.
2-RDD stands for Resilient Distributed Dataset and it is nothing but a distributed collection.
3-It is typically loaded on to the executors created at the time of execution.
4-RDD exposes APIs called as Transformations and Actions
5-Transformations take one RDD as input and return another RDD as output while Actions trigger execution and get data into driver program.
6-Examples of Transformations
    Row Level Transformations – map, filter, flatMap etc
    Aggregations – reduceByKey, aggregateByKey
    Joins – join, leftOuterJoin, rightOuterJoin
    Sorting – sortByKey
    Ranking – groupByKey followed by flatMap with a lambda function
    Except for Row Level Transformations, most of the other transformations have to go through the shuffle phase and trigger new stage.
    Row Level Transformations are also known as Narrow Transformations.
    Transformations that trigger shuffle and new stage are also called as Wide Transformations.

Spark Modules

In the earlier versions of Spark we have core API at the bottom and all the higher level modules work with core API.
Examples of core API are map, reduce, join, groupByKey etc.
But with Spark 2, Data Frames and Spark SQL is becoming the core module.
Core – Transformations and Actions –
  APIs such as map, reduce, join, filter etc. They typically work on RDD
Spark SQL and Data Frames –
  APIs and Spark SQL interface for batch processing on top of Data Frames or Data Sets (not available for Python)
Structured Streaming –
  APIs and Spark SQL interface for stream data processing on top of Data Frames
Machine Learning Pipelines – 
  Machine Learning data pipelines to apply Machine Learning algorithms on top of Data Frames
GraphX Pipelines

Spark Data Structures

We need to deal with 2 types of data structures in Spark –
RDD and Data Frames.  
  RDD is there for quite some time and it is the low level data structure which spark uses to distribute the data
  between tasks while data is being processed
  RDD will be divided into partitions while data being processed. Each partition will be processed by one task.
  Data Frame is nothing but RDD with structure
  Typically we read data from file systems such as HDFS, S3, Azure Blob, Local file system etc
  Based on the file formats we need to use different APIs available in Spark to read data into RDD or Data Frame
  Spark uses HDFS APIs to read and/or write data from underlying file system
  
Spark Framework

these different components of Spark Framework. 
  Driver Program
  Spark Context
  Executors
  Executor Cache
  Executor Tasks
  Job
  Stage
  Task (Executor Tasks)

Following are the different execution modes supported by Spark
  Local (for development)
  Standalone (for development)
  Mesos
  YARN

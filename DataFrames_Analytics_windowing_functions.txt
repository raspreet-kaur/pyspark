from pyspark.sql.functions import *
from pyspark.sql import functions
from pyspark.sql.window import Window


oit = oit.withColumn('order_item_order_id', oit.order_item_order_id.cast(IntegerType())). \
withColumn('order_item_product_id', oit.order_item_product_id.cast(IntegerType())). \
withColumn('order_item_quantity', oit.order_item_quantity.cast(IntegerType())). \
withColumn('order_item_subtotal', oit.order_item_subtotal.cast(FloatType())). \
withColumn('order_item_produc_price', oit.order_item_produc_price.cast(FloatType()))


oit.groupBy('order_item_order_id').agg(sum(oit.order_item_produc_price)).show()

oit.join(revenue, oit.order_item_order_id == revenue.order_item_order_id). \
... select(oit.order_item_order_id, 'rev').show()

# in the above case, we are first reading whole data to self join
#but using following approach we can make it fast, by using over(spec)


###VVIMP
spec = Window.partitionBy(oit.order_item_order_id)

oit.select('order_item_order_id', 'order_item_subtotal', 'order_item_product_id', round(oit.order_item_subtotal / sum(oit.order_item_subtotal).over(spec) * 100, 2). \
alias('revenue')).show()

oit.select('order_item_order_id', 'order_item_subtotal', 'order_item_product_id', round(sum(oit.order_item_subtotal).over(spec), 2). \
alias('revenue')).show()

#help(Window)
partitionBy is used for aggregations
orderBy is used for global sorting.

combination of partitionBy and orderBy is used for ranking and windows function.

##Problem statement -> get the difference between the highest revenue generated by an order and highest -1 revenue.

spec = Window.partitionBy('order_item_order_id').orderBy(oit.order_item_subtotal.desc()) #have to use column in orderBy not string literal
lead = oit.withColumn('next', lead(oit.order_item_subtotal).over(spec)).orderBy('order_item_order_id', oit.order_item_subtotal.desc()).show()


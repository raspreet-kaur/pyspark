
from pyspark.sql.types import *
schema = StructType([
     StructField("order_id", IntegerType(), True),
     StructField("order_date", TimestampType(), True),
     StructField("order_value", FloatType(), True),
     StructField("order_status", StringType(), True)])
	
df = spark.read.csv('/public/retail_db/orders/', schema=schema) #in case table is big, better to use  from pyspark.sql.types import *

df = spark.read.csv('/public/retail_db/orders/',schema='id INT, date TIMESTAMP, val FLOAT, status STRING') #defining column names with types

df = spark.read.csv('/public/retail_db/orders/', inferSchema=True)	#It will take schema from source


/user/randeep89/s.csv ----->contains-----> 1%2013-07-25 00:00:00.0%11599%CLOSED


orders = ordersCSV. \
  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \
  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))

orders.printSchema()
orders.show()

DIFFERENCE BETWEEN SELECT AND WITHCOLUMN IS, SELECT selects mentioned column, whereas withColumn modifies only specified column and carry other columns as is.
